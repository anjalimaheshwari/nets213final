# Components:

## Data Gathering

Difficulty: 1

Pick a genre that will work well with generative models, then find free music websites from which we can download a high volume of music in that genre (in MIDI format)

Split the dataset into training and testing sets

## Finding and Training Generative Models

Difficulty: 3

Browse online for music generation models and find their code. Try to run their model with the training data and fix potential problems. Run each model to generate enough samples for the next step.

## Quality Control/Task Design

Difficulty: 4

Data: the HITs take in music clips generated by the models along with some from the testing set (as a control)

Quality check: programatically add an audio pronunciation of a number to the end of each music clip which the Turker will have to enter into a textbox in the HIT

Design: One music clip per HIT, Turker selects a rating from 1 to 10 of how good the clip sounds and then does the quality check. The slider for playing audio will not be draggable so that the Turker cannot skip the clip without listening to it.

Workers: Limiting workers to the US since we will be using Western music in our data samples, multiple workers will listen to each audio clip

## Creating the HIT

Difficulty: 1

Most of the difficulty of the HIT is in the quality control aspect, the remaining components are simple data collection (1-10 ratings, additional info parameters about the Turker and their music preference, etc.). See mockup for visual example.

## Aggregating data from the HIT

Difficulty: 2

Once we have collected data from all of our HITs, there are a number of parameters across which we will be aggregating the result data. The main one will be ranking the various generative models based on the average ratings of the music they produce and selecting the best model for the second round of HITs.

Additionally, we will aggregate based on other secondary information we ask from the Turkers (music taste, experience with music, how often they listen to music, age/location, etc) for the analysis part.

## Creating new instances of the best model and parameter tuning

Difficulty: 2

Select the best model based on the aggregated ratings

Use multiple instances of the best model with different hyperparameters (or different training data) to generate music which will go through the rating and aggregation processes again to determine the final best model with a certain set of parameters/hyperparameters.

## Analysis

Difficulty: 3

Drawing conclusions based on the data aggregation on which model is most effective and why, and which hyperparameters improved models quality the most, compare model generated music with the control (human generated music) and determine the distribution of worker preference based on the secondary information we gathered.

## Quality Control: 
For quality control, we inputted midi files that were generated from the training models. The output of the quality control component is file paths to the MP3 file. These files contain a music clip that is concatenated with a 2 digit number. This 2 digit number serves as a quality control measure to ensure that the Turker listens until the end. This 2 digit number will also be listed in the output file. Furthermore, the output includes a directory of the file paths to the MP3 files, which will be uploaded to the cloud. The code can be found in 213Project_QC.ipynb notebook. 

## Aggregation: 
We found the average rating for each model and outputted this data to a CSV file. The code for this component can be found in the simple_aggregation.ipynb notebook. 

Documents for the flow diagram and the mock up can be found at the following: docs/flow-diagram.pdf, docs/mock_up.png.
The sample input and output data can be found at data/QC_sample_input.xlsx, data/QC_sample_output.csv, and data/QC_sample_output.xlsx.
Sample input for aggregation can be found at: data/simple_aggregation.xlsx

