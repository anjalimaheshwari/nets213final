# Components:

## Data Gathering

Difficulty: 1

Pick a genre that will work well with generative models, then find free music websites from which we can download a high volume of music in that genre (in MIDI format)

Split the dataset into training and testing sets

## Finding and Training Generative Models

Difficulty: 3

Browse online for music generation models and find their code. Try to run their model with the training data and fix potential problems. Run each model to generate enough samples for the next step.

## Quality Control/Task Design

Difficulty: 4

Data: the HITs take in music clips generated by the models along with some from the testing set (as a control)

Quality check: programatically add an audio pronunciation of a number to the end of each music clip which the Turker will have to enter into a textbox in the HIT

Design: One music clip per HIT, Turker selects a rating from 1 to 10 of how good the clip sounds and then does the quality check. The slider for playing audio will not be draggable so that the Turker cannot skip the clip without listening to it.

Workers: Limiting workers to the US since we will be using Western music in our data samples, multiple workers will listen to each audio clip

## Creating the HIT

Difficulty: 1

Most of the difficulty of the HIT is in the quality control aspect, the remaining components are simple data collection (1-10 ratings, additional info parameters about the Turker and their music preference, etc.). See mockup for visual example.

## Aggregating data from the HIT

Difficulty: 2

Once we have collected data from all of our HITs, there are a number of parameters across which we will be aggregating the result data. The main one will be ranking the various generative models based on the average ratings of the music they produce and selecting the best model for the second round of HITs.

Additionally, we will aggregate based on other secondary information we ask from the Turkers (music taste, experience with music, how often they listen to music, age/location, etc) for the analysis part.

## Creating new instances of the best model and parameter tuning

Difficulty: 2

Select the best model based on the aggregated ratings

Use multiple instances of the best model with different hyperparameters (or different training data) to generate music which will go through the rating and aggregation processes again to determine the final best model with a certain set of parameters/hyperparameters.

## Analysis 

Difficulty: 3

Drawing conclusions based on the data aggregation on which model is most effective and why, and which hyperparameters improved models quality the most, compare model generated music with the control (human generated music) and determine the distribution of worker preference based on the secondary information we gathered.

## How to Use Noteworthy
Students must accept the HIT on the Amazon MTurk Worker Sandbox using the link: https://workersandbox.mturk.com/projects/3TCGLRK80L0CG7BRUKYSENZ2INWZQX/tasks?ref=w_pl_prvw

Noteworthy asks students to listen to two music recordings and rate if these recordings should be concatenated. The instructions at the top of the screen ask the student to listen to the original music selection, the primer, in its entirety. We understand that this recording may be long, thus, when listening to the second recording, so we have included the last 15 seconds of the original music to the new recording. After listening to this recording, the student must rate from 1 to 5 (5 being the best) how well the new recording fits with the original. 

Furthermore, there will be a 2-digit number at the end of the recording to ensure quality control. The student must type the correct code in the textbox in order for the task to be accepted. 

If anyone has any questions, please email: nwthyinfo@gmail.com. 

Thanks for using Noteworthy!

## Raw Data

<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
The music files we will be training on are from the Lahk MINI Dataset (LMD) and have been matched using the Million Song Dataset (MSD). Furthermore, we are using a subset of the data that are only piano rolls, have at most one time signature change events, have a 4/4 time signature, and start at time zero. This dataset has 21,425 multitrack pianorolls with no duplicates. The song are in midi format.
=======
3 sample training music midi files are included in data/.
>>>>>>> parent of 079f209 (Update README.md)
=======
3 sample training music midi files are included in data/.
>>>>>>> parent of 079f209 (Update README.md)
=======
3 sample training music midi files are included in data/.
>>>>>>> parent of 079f209 (Update README.md)

## Quality Control: 
In the notebook, 213_prototype_performanceRNN, we use the Magenta Model, which uses deep learning algorithms to generate songs. This model uses TensorFlow and is open source. For quality control, the generated music in the form of a list of midi files is in the directory, "model_out", and will be taken as the input to the QC module. For each midi file, it will be converted into audio mp3 and a short audio auto-generated by text-to-speech from the text "Please enter XX into the textbox", where XX is a random 2 digit number. The combined final audio .mp3 files will be stored in a directory, "QC_out/mp3_out". A CSV file, "QC_out.csv" will also be created at the end which contains the filepaths to the MP3 files and the corresponding 2 digit number within each audio. The "QC_out/mp3_out" containing MP3 files will be uploaded to Github and the HIT will take in only the filepaths and the 2-digit numbers and play each audio from the url filepath, and check for a match between the textbox input and the provided 2-digit number. 

## Aggregation: 
Because the evaluation of music is a subjective matter, we simply take the average rating for each song as its final rating, which can be done during aggregation.

The code for this component can be found in the simple_aggregation.ipynb. 

## Analysis
We will use multiple workers to rate each song and take the average as the final rating for the song. We will have 3 iterations, in which we will take the recordings with the best ratings and append them to the primer. This will serve as the input for the next iteration. Thus, the original selection will grow by 15 seconds in each round and the songs will be a length of (primer + 45 seconds) by the final round. After the final round, we will output the highest-rated songs. 

Documents for the flow diagram and the mock up can be found at the following: docs/flow-diagram.pdf, docs/mock_up.png.
The sample input and output data can be found at data/QC_sample_input.xlsx, data/QC_sample_output.csv, and data/QC_sample_output.xlsx.
Sample input for aggregation can be found at: data/simple_aggregation.xlsx
