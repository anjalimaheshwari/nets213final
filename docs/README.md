# Components:

## Data Gathering

Difficulty: 1

Pick a genre that will work well with generative models, then find free music websites from which we can download a high volume of music in that genre (in MIDI format)

Split the dataset into training and testing sets

## Finding and Training Generative Models

Difficulty: 3

Browse online for music generation models and find their code. Try to run their model with the training data and fix potential problems. Run each model to generate enough samples for the next step.

## Quality Control/Task Design

Difficulty: 4

Data: the HITs take in music clips generated by the models along with some from the testing set (as a control)

Quality check: programatically add an audio pronunciation of a number to the end of each music clip which the Turker will have to enter into a textbox in the HIT

Design: One music clip per HIT, Turker selects a rating from 1 to 10 of how good the clip sounds and then does the quality check. The slider for playing audio will not be draggable so that the Turker cannot skip the clip without listening to it.

Workers: Limiting workers to the US since we will be using Western music in our data samples, multiple workers will listen to each audio clip

## Creating the HIT

Difficulty: 1

Most of the difficulty of the HIT is in the quality control aspect, the remaining components are simple data collection (1-10 ratings, additional info parameters about the Turker and their music preference, etc.). See mockup for visual example.

## Aggregating data from the HIT

Difficulty: 2

Once we have collected data from all of our HITs, there are a number of parameters across which we will be aggregating the result data. The main one will be ranking the various generative models based on the average ratings of the music they produce and selecting the best model for the second round of HITs.

Additionally, we will aggregate based on other secondary information we ask from the Turkers (music taste, experience with music, how often they listen to music, age/location, etc) for the analysis part.

## Creating new instances of the best model and parameter tuning

Difficulty: 2

Select the best model based on the aggregated ratings

Use multiple instances of the best model with different hyperparameters (or different training data) to generate music which will go through the rating and aggregation processes again to determine the final best model with a certain set of parameters/hyperparameters.

## Analysis

Difficulty: 3

Drawing conclusions based on the data aggregation on which model is most effective and why, and which hyperparameters improved models quality the most, compare model generated music with the control (human generated music) and determine the distribution of worker preference based on the secondary information we gathered.

## Raw Data

The music files we will be training on are from the Lahk MINI Dataset (LMD) and have been matched using the Million SOng Dataset (MSD). Furthermore, we are using a subset of the data that are only piano rolls, have at most one time signature change events, have a 4/4 time signature, and start at time zero. This dataset has 21,425 multitrack pianorolls with no duplicates. The song are in midi format.

## Quality Control: 
For quality control, the generated music in the form of a list of midi files will be taken as the input to the QC module. For each midi file, it will be converted into audio mp3 and a short audio auto-generated by text-to-speech from the text "Please enter XX into the textbox", where XX is a random 2 digit number. The combined final audio .mp3 files will be stored in a directory called "outdata", and an "output_file.csv" will also be created at the end which contains the filepaths to the .mp3 files and the corresponding 2 digit number within each audio. The "outdata" containing .mp3 files will most likely be uploaded to the cloud or somewhere that can turn them into urls (in case they are not able to be uploaded to MTurk for whatever reason) and the HIT will take in only the filepaths and the 2-digit numbers and play each audio from the url filepath, and check for a match between the textbox input and the provided 2-digit number. In the final version, we may need to update the filenames to the urls. The code 213Project_QC.ipynb currently creates the list of midi files itself by taking directly from the training dataset. We will actaully also include these as controls for our experiment. Because the evaluation of music is a subjective matter, we simply take the average rating for each model as its final rating, which can be done during aggregation.

## Aggregation: 

The code for this component can be found in the simple_aggregation.ipynb. 

Documents for the flow diagram and the mock up can be found at the following: docs/flow-diagram.pdf, docs/mock_up.png.
The sample input and output data can be found at data/QC_sample_input.xlsx, data/QC_sample_output.csv, and data/QC_sample_output.xlsx.
Sample input for aggregation can be found at: data/simple_aggregation.xlsx

